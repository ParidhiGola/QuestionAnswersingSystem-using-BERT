{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question_Answer_Covid.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNmuYOqi87l2ZC1x9LgUhbn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j6eVkw4O917E"},"source":["*Construct Question Answering System capable of answering questions related to corona virus specifically questions like What are symptoms of corona, What do we know about COVID-19 risk factors, How to prevent corona etc. i.e. some corona literature based questions.*"]},{"cell_type":"markdown","metadata":{"id":"c-ur89rXIARn"},"source":["https://medium.com/@aakashgoel12/question-answering-system-on-corona-approach-01-6ef9799695cb"]},{"cell_type":"markdown","metadata":{"id":"49PU8_KxHp_j"},"source":["naive approach to solve the problem"]},{"cell_type":"markdown","metadata":{"id":"vqfdXDL2H4Jz"},"source":["Lets start on below idea for our 1st end to end solution:\n","Download all corona articles and simply split it into sentences.<>\n","\n","Filter relevant sentences based on userâ€™s query.\n","Feed each relevant sentence as context to BERT Model fine-tuned on SQUAD data-set and show selected answers based on score given by model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Pwzn83w93RZ","executionInfo":{"status":"ok","timestamp":1611864940518,"user_tz":480,"elapsed":567,"user":{"displayName":"Paridhi Bhargav","photoUrl":"","userId":"17325204801395574142"}},"outputId":"e4b9e010-63ef-488b-98b0-256035dbb2a3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y5C9ICe3SyDF"},"source":["### 1.Download data from Kaggle:"]},{"cell_type":"code","metadata":{"id":"xaz5rvAuKLNM"},"source":[" ! pip install -q kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":89},"id":"Il572qa0LMhF","executionInfo":{"status":"ok","timestamp":1611864952507,"user_tz":480,"elapsed":12547,"user":{"displayName":"Paridhi Bhargav","photoUrl":"","userId":"17325204801395574142"}},"outputId":"ac4f4370-4f75-46be-a58f-ea46056ae7f6"},"source":["from google.colab import files\n","files.upload()\n","\n","#/content/kaggle.json"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-57a4d546-e90f-4aff-9bde-1e8a8e457e0c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-57a4d546-e90f-4aff-9bde-1e8a8e457e0c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving kaggle.json to kaggle.json\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"paridhigola\",\"key\":\"d2c256f2334c666285afa5ce0ccc66df\"}'}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"norHj1wBLoAu"},"source":["! mkdir ~/.kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_tqUpv1LklZ"},"source":["! cp kaggle.json ~/.kaggle/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ryo1JRDHKPjt"},"source":["! chmod 600 ~/.kaggle/kaggle.json\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phMP5raEKyYE"},"source":["#!kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge -p /content/drive/MyDrive/Question_Answering_System/COVID-19"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYP2WgftMXZQ"},"source":["#! unzip /content/drive/MyDrive/Question_Answering_System/COVID-19/CORD-19-research-challenge.zip -d /content/drive/MyDrive/Question_Answering_System/COVID-19"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrPufa_ZKyao","executionInfo":{"status":"ok","timestamp":1611865012847,"user_tz":480,"elapsed":72864,"user":{"displayName":"Paridhi Bhargav","photoUrl":"","userId":"17325204801395574142"}},"outputId":"edecb1d5-b1b9-481d-9e81-d2cbdac74757"},"source":["## Storing file absolute path in list\n","from tqdm import tqdm\n","tqdm.pandas()\n","import pandas as pd\n","import json,os,gc\n","\n","\n","#path1 = 'biorxiv_medrxiv/biorxiv_medrxiv/'\n","#path2 = 'comm_use_subset/comm_use_subset/'\n","#path3 = 'custom_license/custom_license/'\n","#path4 = 'noncomm_use_subset/noncomm_use_subset/'\n","\n","\n","#paths = [path1,path2,path3,path4]\n","path = '/content/drive/MyDrive/Question_Answering_System/COVID-19/document_parses/pdf_json/'\n","file_names = []\n","#for path in paths:\n","  #temp_file_names = os.listdir(path)\n","  #file_names.extend([path+file_name for file_name in temp_file_names])\n","\n","\n","temp_file_names = os.listdir(path)\n","for file_name in temp_file_names:\n","  file_names.extend([path+file_name])\n","len(file_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12017"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"0kWz4pPhbWKF"},"source":["#nltk.download('all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDI92tTqSsi_"},"source":["### 2. Parse data\n"," Downloaded articles are in .json format, need to convert all data into rows and columns where each rows signify one article."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"qkDZyEUOLtb1","executionInfo":{"status":"error","timestamp":1611866932325,"user_tz":480,"elapsed":2597,"user":{"displayName":"Paridhi Bhargav","photoUrl":"","userId":"17325204801395574142"}},"outputId":"0538f81b-8c9b-463e-f0d0-fd6ed81c863f"},"source":["## function to parse data\n","from time import time\n","from copy import deepcopy\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk import sent_tokenize,word_tokenize\n","from nltk.corpus import wordnet,stopwords\n","from nltk.stem import WordNetLemmatizer\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","tqdm.pandas()\n","import os,re,multiprocessing,joblib\n","from multiprocessing import Pool\n","from collections import defaultdict\n","\n","\n","stopwords = set(stopwords.words('english'))\n","# nltk.download('all')\n","\n","\n","def file_content(file_path):\n","  abstract='';body_text = '';error_count = 0\n","  if os.path.splitext(file_path)[1]=='.json':\n","    f = open(file_path)\n","    f_json = json.load(f)\n","    try:\n","      abstract = f_json['abstract'][0]['text']\n","    except:\n","      error_count+=1\n","    for i in f_json['body_text']:\n","      try:\n","        body_text= body_text+' '+i['text']\n","      except:\n","        error_count+=1\n","    body_text = body_text.strip()\n","    f.close()\n","    return body_text,abstract,error_count\n","  else:\n","    return body_text,abstract,error_count\n","\n","## Storing article and related information in data-frame\n","df = pd.DataFrame({'file_name':[],'body':[],'abstract':[],'error_count':[]})\n","df['file_name'] = file_names\n","df['article_no'] = list(range(df.shape[0]))\n","for ind,info in tqdm(df.iterrows(),total=df.shape[0]):  df.loc[ind,'body'],df.loc[ind,'abstract'],df.loc[ind,'error_count'] = \\\n","  file_content(file_path=info['file_name'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e6a880b23daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m## Storing article and related information in data-frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'error_count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_no'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'error_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mfile_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'file_names' is not defined"]}]},{"cell_type":"code","metadata":{"id":"3NzCTjq_Ya1f"},"source":["df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXEl3-P3ZTxK"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxmKpf5eWdoj"},"source":["Now, all articles stored in data-frame with columns\n","file_name: containing absolute file path (unique)\n",">1. **body:** Content of article<br>\n",">2. **abstract:** Abstract of article<br>\n",">3. **error_count:** Continuous value tells if any error came while parsing the file<br>\n",">4. **article_no:** Unique Identification number for each article, simply number starting from 0"]},{"cell_type":"markdown","metadata":{"id":"pRIXWbOlXqbd"},"source":["### 3. Preprocessing functions\n","Below is code for some common preprocessing (lemmatization, stopword removal) steps used"]},{"cell_type":"code","metadata":{"id":"9gvCtL8XWeWF"},"source":["corpus_file = 'corpus.txt'\n","sent_dict_file = 'sent.joblib.compressed'\n","word_sent_no_dict_file = 'word_sent_no.joblib.compressed'\n","orig_word_sent_no_dict_file = 'orig_word_sent_no.joblib.compressed'\n","stopword_file = 'stopword.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvxtPSmcKydL"},"source":["## Lemmatization function\n","def get_wordnet_pos(word):\n","  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","  tag = nltk.pos_tag([word])[0][1][0].upper()\n","  tag_dict = {\"J\": wordnet.ADJ,\n","              \"N\": wordnet.NOUN,\n","              \"V\": wordnet.VERB,\n","              \"R\": wordnet.ADV}\n","  return tag_dict.get(tag, wordnet.NOUN)\n","\n","# 1. Init Lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","def get_lemmatize(sent):\n","  return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(sent)])\n","\n","def parallelize_dataframe(df, func, num_partitions, num_cores):\n","  df_split = np.array_split(df, num_partitions)\n","  pool = Pool(num_cores)\n","  df = pd.concat(pool.map(func, df_split))\n","  pool.close()\n","  pool.join()\n","  return df\n","  \n","def fn_lemmatize(data):\n","  for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n","    data.loc[ind,'sentence_lemmatized'] = get_lemmatize(sent = info['sentence'])\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slbqVGCWXtpV"},"source":["## removing stopwords\n","def words(text): \n","  return re.findall(r'\\w+', text.lower())\n","  \n","#stopwords = list(set(words(open(stopword_file).read())))\n","\n","def remove_stopwords(sent):\n","  ## case conversion - lower case\n","  word_tokens = words(text=sent)\n","  #sent = sent.lower()\n","  #word_tokens = word_tokenize(sent)\n","  ## removing stopwords\n","  filtered_sentence = \" \".join([w for w in word_tokens if not w in stopwords])\n","  ## removing digits\n","  filtered_sentence = re.sub(r'\\d+','',filtered_sentence)\n","  ## removing multiple space\n","  filtered_sentence = words(text = filtered_sentence)\n","  return \" \".join(filtered_sentence)\n","\n","def fn_stopword(data):\n","  for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n","    sent = info['sentence_lemmatized']\n","    data.loc[ind,'sentence_lemma_stop'] = remove_stopwords(sent)\n","  return data\n","\n","def fn_stopword_orig(data):\n","  for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n","    sent = info['sentence']\n","    data.loc[ind,'sentence_stop'] = remove_stopwords(sent)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cIIt-cw5Xtr4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JkajUD0ashz"},"source":["### 4. Convert data in to useful format\n",">**Sentence dictionary**: Split all body and abstract from above dataframe into sentences using ```nltk.sent_tokenize``` and store in dictionary where sentence number is key and value is sentence. It will be used to retrieve sentences given sentence number."]},{"cell_type":"code","metadata":{"id":"_eSNLqx_auMz"},"source":["## creating sentence dictionary\n","df['article'] = df['body']+' '+ df['abstract']\n","df['article'].fillna('',inplace=True)\n","article_no_sent_dict = dict()\n","\n","for ind,info in tqdm(df.iterrows(),total=df.shape[0]):\n","  article_no_sent_dict[info['article_no']] = sent_tokenize(str(info['article']))\n","  \n","article_no_list = list();sent_list = list()\n","df_sent = pd.DataFrame({'article_id':[],'sentence':[]})\n","for i in tqdm(article_no_sent_dict,total=len(article_no_sent_dict)):\n","  article_no_list.extend([i]*len(article_no_sent_dict[i]))\n","  sent_list.extend(article_no_sent_dict[i])\n","df_sent['article_id'] = article_no_list ; df_sent['sentence'] = sent_list\n","df_sent['sent_no'] = list(range(df_sent.shape[0]))\n","## sentence level dictionary\n","sent_dict = dict()\n","for ind,info in tqdm(df_sent.iterrows(),total=df_sent.shape[0]):\n","  sent_dict[info['sent_no']] = info['sentence']\n","sent_dict[-1] = 'NULL'\n","sent_dict_file = 'sent.joblib.compressed'\n","joblib.dump(sent_dict,sent_dict_file, compress=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrN1qRMHauPS"},"source":["df_sent.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O12gxFGdauSM"},"source":["df_sent.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHYmbvjxcDvf"},"source":["Lemmatized Word â€” Sentence No. Inverse dictionary: <br>Create dictionary with key as lemmatized word from sentence and value as list of sentence number where lemmatized word is present. It will be used to retreive list of sentence number given lemmatized word."]},{"cell_type":"code","metadata":{"id":"MRJcoRVvauVF"},"source":["## lemmatization over sentence\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","df1 = deepcopy(df_sent)\n","df1 = parallelize_dataframe(df=df1, func=fn_lemmatize, num_partitions=27, num_cores=27)\n","## removing stopword from lemmatized sentence\n","df1 = parallelize_dataframe(df=df1, func=fn_stopword, num_partitions=30, num_cores=35)\n","## saving inverse dictionary on lemmatized sentence i.e. word and sentence no\n","word_sent_no_dict = defaultdict(list)\n","for ind,info in tqdm(df1.iterrows(),total=df1.shape[0]):\n","  sent_words = words(info['sentence_lemma_stop'])\n","  for w in sent_words:\n","    word_sent_no_dict[w].append(info['sent_no'])\n","joblib.dump(word_sent_no_dict,word_sent_no_dict_file, compress=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uduDOlGq6jB"},"source":["### **Original Word **â€” Sentence No.<br>\n"," Inverse dictionary: Create dictionary with key as original word from sentence and value as list of sentence number where lemmatized word is present. It will be used to retreive list of sentence number given original word."]},{"cell_type":"code","metadata":{"id":"l8aEAJW5q7HC"},"source":["## saving inverse dictionary on original sentence i.e. word and sentence no\n","df1 = deepcopy(df_sent)\n","df1 = parallelize_dataframe(df=df1, func=fn_stopword_orig, num_partitions=35, num_cores=35)\n","orig_word_sent_no_dict = defaultdict(list)\n","for ind,info in tqdm(df1.iterrows(),total=df1.shape[0]):\n","  sent_words = words(info['sentence_stop'])\n","  for w in sent_words:\n","    orig_word_sent_no_dict[w].append(info['sent_no'])\n","joblib.dump(orig_word_sent_no_dict,orig_word_sent_no_dict_file, compress=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TX02qU79cb_h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvmW-4HSqTbg"},"source":["### **Corpus.txt:** It contains all sentences in single .txt file, will be used as reference for query spelling correction purpose."]},{"cell_type":"code","metadata":{"id":"msfsVzv_qU3A"},"source":["## Corpus - for spelling correction model\n","outF = open(corpus_file, \"w\")\n","for line in tqdm(df_sent['sentence'],total=df_sent.shape[0]):\n"," # write line to output file\n"," outF.write(line)\n"," outF.write(\"\\n\")\n","outF.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"npgxvcExqU5Z"},"source":["import os\n","os.listdir('.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MyTOBlisIxy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HS5XlYRzsMFd"},"source":["### 5. Query Processing System"]},{"cell_type":"code","metadata":{"id":"ZfTiqihtsM6O"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"icBzraoRsNG2"},"source":["### 6. Filter relevant Sentences based on Query"]},{"cell_type":"code","metadata":{"id":"lEbEmS-tsSqF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZzZSewfsSsg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oiQHlxZAsdUU"},"source":["### 7. Use BERT Model for QA System"]},{"cell_type":"code","metadata":{"id":"a_vYyjocsewS"},"source":["!pip install transformers\n","from transformers import pipeline\n","\n","\n","nlp = pipeline('question-answering',model = 'bert-large-cased-whole-word-masking-finetuned-squad')\n","\n","query_sample = \"How to prevent Corona ?\"\n","relevant_sentence = \"When asked why they were wearing masks, several students answered that they were 'preventing corona'.\"\n","\n","\n","nlp(question = query_sample, context = relevant_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgbYUQdtse07"},"source":["#query_sample = input(\"Enter question:\") \n","#print(query_sample) \n","#nlp(question = query_sample)"],"execution_count":null,"outputs":[]}]}